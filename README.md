## üìù Description
Hi All!

Classification is a machine learning task that involves predicting categorical class labels for new instances by learning from a dataset with input features and corresponding labels. The main aim is to develop a model that can learn patterns from the training data and effectively predict class labels for new, unseen data.

We will use several classification algorithms to predict how likely customers are going to purchase a new car, which can be a highly valuable and effective approach, given the right circumstances and data quality. In this case, we will run experiments using 6 algorithms:

1. Logistic Regression
2. SVC
3. KNN
4. DecisionTree
5. RandomForest
6. ExtraTrees

Some of the techniques we will be using include:

1. Data preparation/cleaning
2. Feature engineering
3. Regularization (L1/L2)
4. Hyperparameter tuning
5. Automated hyperparameter tuning
6. Features importance

We will use CRISP-DM framework to provide a more structured approach to tackle this project, properly documents every step taken and stay focused and organized on our objectives throughout the process.

Six major phases of CRISP-DM:

1. Business Understanding
2. Data Understanding
3. Data Preparation
4. Modelling
5. Evaluation
6. Deployment

## üë©‚Äçüíª Experiments

The experiments are broken down into 6 python notebooks, each notebook will use 1 algorithm containing its own data cleaning, data preparation, feature engineering, training, testing processes.

Experiment 1: [Logistic Regression](/Experiments/Logistic_Regression.ipynb)\
A popular statistical technique that is relatively simple, interpretable, flexible and computationally efficient

Experiment 2: [KNN](/Experiments/KNN.ipynb)\
Flexible algorithm and useful in cases where data is non-linear and/or has more complex feature relationships, although it has a higher computation cost

Experiment 3: [SVC](/Experiments/SVC.ipynb)\
Powerful algorithm for classification problems especially with dataset containing noisy data or complex feature relationships

Experiment 4: [Decision Tree](/Experiments/DecisionTree.ipynb)\
Useful and flexible algorithm for machine learning applications, can handle highly non-linear data or has complex feature relationships

Experiment 5: [RandomForest](/Experiments/RandomForest.ipynb)\
A collection of decision trees trained on different subsets of data and features. It aggregates the prediction outcomes of all individual decision trees for its final prediction, which can be computationally expensive.

Experiment 6: [ExtraTrees](/Experiments/ExtraTrees.ipynb)\
A variation of Random Forest aiming to reduce the model variance by increasing the randomness of individual trees, the downside is that the randomness can lead to difficulty in interpreting the model.

[Detailed report](/project_summary.pdf): contains detailed summary explanation of every steps taken from business objective, data preparation, training results to final recommendation.

## ‚öôÔ∏è How to Setup

1. Download the experiments (6 Python notebooks), for links, see above section (Experiments)
2. Open the notebooks using local editor (vscode, Pycharm etc) or Jupyter notebook
3. Run the code chunks from the top (dataset will be downloaded automatically)
4. Analyze the results
5. For more detailed explanation, download the detailed report and evaluation [here](/project_summary.pdf)

## üë® Feedback/Ideas? Lets connect!

I would love to hear feedbacks or ideas from you! Or just simply connect and chat, feel free to contact me on:

<a href="https://www.linkedin.com/in/michaelyaputra/">
    <img align="left" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg"/>

</a>

<a href="https://github.com/mcyaputra">
    <img align="left" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/github.svg" />

</a>